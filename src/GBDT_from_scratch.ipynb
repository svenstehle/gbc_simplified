{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright &copy; 2022, Sven Stehle.\n",
    "\n",
    "All rights reserved.\n",
    "\n",
    "This source code is licensed under the BSD-style license found in the\n",
    "LICENSE file in the root directory of this source tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# set random state and number of classes K\n",
    "rng = 42\n",
    "n_classes = 4\n",
    "\n",
    "# dummy data for classification\n",
    "X, y = make_classification(\n",
    "    n_samples=500,\n",
    "    n_features=12,\n",
    "    n_informative=8,\n",
    "    n_redundant=2,\n",
    "    n_repeated=2,\n",
    "    n_classes=n_classes,\n",
    "    random_state=rng,\n",
    ")\n",
    "\n",
    "# split the data, so we can get a good performance estimate on the test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "  X, y, test_size=0.2, random_state=rng\n",
    ")\n",
    "\n",
    "# scale the data\n",
    "# for the sake of brevity, that is all that we will do with this data\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "\n",
    "class GradientBoostingCustom:\n",
    "    def __init__(\n",
    "      self, n_estimators=30, learning_rate=0.1, max_depth=3, random_state=42\n",
    "    ):\n",
    "        \"\"\"Constructor of GradientBoostingCustom. During init,\n",
    "        self.base_estimator will be set as an instance of DummyClassifier.\n",
    "        During the call to ``fit()``, self.base_estimator will become the\n",
    "        root of the gradient boosting ensemble. It is the base for the first \n",
    "        ``outputs``, which are log-probabilities for each of the K classes.\n",
    "        Based on these ``outputs``, we build the ensemble with each new \n",
    "        boosting stage and further refine the ``outputs``.\n",
    "        Args:\n",
    "            n_estimators (int, optional): the number of boosting stages. \n",
    "              Defaults to 30.\n",
    "            learning_rate (float, optional): the influence of each new \n",
    "              estimator ``i`` on the current ``outputs`` of stage \n",
    "              ``i-1``. Defaults to 0.1.\n",
    "            max_depth (int, optional): the maximum depth of each \n",
    "              DecisionTreeRegressor. Should be tuned. \n",
    "              Note: we do not have to work with stumps here. Defaults to 3.\n",
    "            random_state (int, optional): the seed for the random number\n",
    "              generator. Defaults to 42.\n",
    "        \"\"\"\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.random_state = random_state\n",
    "        self.ensemble = None\n",
    "        self.classes_ = None\n",
    "        self.n_classes_ = None\n",
    "        self.base_estimator = DummyClassifier(strategy=\"prior\")\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
    "        \"\"\"Fits the algorithm on the input features ``X``\n",
    "        and the Target labels ``y``.\n",
    "        Args:\n",
    "            X (np.ndarray of shape (n_samples, K)): input features.\n",
    "            y (np.ndarray of shape (n_samples, )): Target labels.\n",
    "        Returns:\n",
    "            GradientBoostingCustom: instance of self.\n",
    "        \"\"\"\n",
    "        random_state = np.random.RandomState(self.random_state)\n",
    "\n",
    "        # get fit base estimator and initial outputs\n",
    "        self.base_estimator.fit(X, y)\n",
    "        outputs = self._get_initial_outputs(X, self.base_estimator)\n",
    "\n",
    "        # get the class information\n",
    "        self.classes_ = np.unique(y)\n",
    "        self.n_classes_ = len(self.classes_)\n",
    "\n",
    "        # create an array that holds ``n_estimators`` for each \n",
    "        # ``n_classes`` (\"ensemble of trees\")\n",
    "        self.ensemble = np.empty(\n",
    "          (self.n_estimators, self.n_classes_), dtype=object\n",
    "        )\n",
    "\n",
    "        # compute the residuals for each boost stage and class\n",
    "        for iboost in range(self.n_estimators):\n",
    "            for k in range(self.n_classes_):\n",
    "                # get the negative gradient (residuals) for each class\n",
    "                residuals = self._compute_residuals(y, outputs, k)\n",
    "\n",
    "                # fit regression tree on residuals\n",
    "                tree = DecisionTreeRegressor(\n",
    "                  max_depth=self.max_depth,\n",
    "                  splitter=\"best\",\n",
    "                  random_state=random_state,\n",
    "                )\n",
    "                tree.fit(X, residuals)\n",
    "\n",
    "                # update predictions for class k with current regression tree\n",
    "                outputs[:, k] = self._update_outputs(X, outputs, k, tree)\n",
    "\n",
    "                # add tree to ensemble\n",
    "                self.ensemble[iboost, k] = tree\n",
    "\n",
    "        return self\n",
    "\n",
    "    def _compute_residuals(self, y: np.ndarray, outputs: np.ndarray, k: int):\n",
    "        \"\"\"Compute residuals for class k.\n",
    "        Residuals can be understood as negative gradient.\n",
    "        They are expressed as the directional error\n",
    "        and the magnitude of the necessary change in probability\n",
    "        to correctly predict each respective label with\n",
    "        regard to class k.\n",
    "        Args:\n",
    "            y (np.ndarray of shape (n_samples, )): Target labels.\n",
    "            outputs (np.ndarray of shape (n_samples, K)): The outputs as\n",
    "                log-probabilities of the tree ensemble at\n",
    "                iteration ``i - 1`` for all ``K`` classes.\n",
    "            k (int): index of the class.\n",
    "        Returns:\n",
    "            np.ndarray of shape (n_samples, ): negative gradient for class k.\n",
    "        \"\"\"\n",
    "        # encode y as array of [0, 1]. 1 if element equals k, 0 otherwise\n",
    "        y = np.where(y == k, 1, 0)\n",
    "\n",
    "        # get outputs for class k and convert to clipped probabilities\n",
    "        log_probas_class_k = outputs[:, k]\n",
    "        probas_class_k = np.exp(log_probas_class_k)\n",
    "        clipped_probas_class_k = np.nan_to_num(probas_class_k)\n",
    "\n",
    "        # compute and return residuals (negative gradient)\n",
    "        negative_gradient = y - clipped_probas_class_k\n",
    "        return negative_gradient\n",
    "\n",
    "    def _update_outputs(\n",
    "        self,\n",
    "        X: np.ndarray,\n",
    "        outputs: np.ndarray,\n",
    "        k: int,\n",
    "        tree: DecisionTreeRegressor,\n",
    "    ):\n",
    "        \"\"\"Updates the outputs with the predictions for the residuals\n",
    "        of the current boosting stage.\n",
    "        Args:\n",
    "            X (np.ndarray of shape (n_samples, n_features)): input features.\n",
    "            outputs (np.ndarray of shape (n_samples, K)): The outputs as\n",
    "                log-probabilities of the tree ensemble at\n",
    "                iteration ``i - 1`` for all ``K`` classes.\n",
    "            k (int): index of the class.\n",
    "            tree (DecisionTreeRegressor): the tree that was fit \n",
    "                during the current stage.\n",
    "        Returns:\n",
    "            np.ndarray of shape (n_samples, ): the updated outputs for class k.\n",
    "        \"\"\"\n",
    "        return outputs[:, k].ravel() + self.learning_rate * tree.predict(X)\n",
    "\n",
    "    def _get_initial_outputs(self, X: np.ndarray, estimator: DummyClassifier):\n",
    "        \"\"\"Returns the initial outputs: DummyClassifier ``predict_proba`` always\n",
    "        returns the empirical class distribution of y also known as the\n",
    "        empirical class prior distribution. These probabilities are then clipped\n",
    "        and returned as log-probabilities.\n",
    "        Args:\n",
    "            X (np.ndarray of shape (n_samples, n_features)): input features.\n",
    "            estimator (DummyClassifier): the DummyClassifier object that will\n",
    "                be fit during this function call.\n",
    "        Returns:\n",
    "            np.ndarray of shape (n_samples, K): The outputs as\n",
    "                log-probabilities of the tree ensemble at\n",
    "                iteration 0 for all ``K`` classes.\n",
    "        \"\"\"\n",
    "        # initialize outputs as log-probabilities\n",
    "        probas = estimator.predict_proba(X)\n",
    "        eps = np.finfo(np.float32).eps\n",
    "        probas = np.clip(probas, eps, 1 - eps)\n",
    "        outputs = np.log(probas).astype(np.float64)\n",
    "        return outputs\n",
    "\n",
    "    def predict(self, X: np.ndarray):\n",
    "        \"\"\"Returns the predictions for input features X.\n",
    "        Args:\n",
    "            X (np.ndarray of shape (n_samples, n_features)): input features.\n",
    "        Returns:\n",
    "            np.ndarray of shape (n_samples, ): the predictions for X.\n",
    "        \"\"\"\n",
    "        classes = self.classes_[:, np.newaxis]\n",
    "        outputs = self._get_initial_outputs(X, self.base_estimator)\n",
    "\n",
    "        # make predictions for each of the k classes with the respective\n",
    "        # estimators\n",
    "        for i in range(self.n_estimators):\n",
    "            for k in classes:\n",
    "                # apply decision function to each class\n",
    "                outputs[:, k] = self._update_outputs(\n",
    "                    X,\n",
    "                    outputs,\n",
    "                    k,\n",
    "                    self.ensemble[i, k][0],\n",
    "                ).reshape(-1, 1)\n",
    "\n",
    "        # convert log-probabilities to probability\n",
    "        proba = np.exp(outputs)\n",
    "        # get class label for max probability per row\n",
    "        return np.argmax(proba, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "plt.rcParams['axes.facecolor'] = 'none'\n",
    "\n",
    "number_of_base_learners = 50\n",
    "\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "fig.patch.set_facecolor('white')\n",
    "ax0 = fig.add_subplot(111)\n",
    "accuracies_custom = []\n",
    "accuracies_sklearn = []\n",
    "accuracies_sklearn_rf = []\n",
    "learning_rate = 0.5\n",
    "max_depth = 3\n",
    "\n",
    "for i in range(1, number_of_base_learners + 1):\n",
    "    # custom GBC\n",
    "    model = GradientBoostingCustom(\n",
    "        n_estimators=i,\n",
    "        learning_rate=learning_rate,\n",
    "        max_depth=max_depth,\n",
    "        random_state=rng,\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    accuracies_custom.append(acc)\n",
    "\n",
    "    # sklearn GBC\n",
    "    model = GradientBoostingClassifier(\n",
    "        n_estimators=i,\n",
    "        learning_rate=learning_rate,\n",
    "        max_depth=max_depth,\n",
    "        random_state=rng,\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    accuracies_sklearn.append(acc)\n",
    "\n",
    "    # sklearn RF\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=i,\n",
    "        random_state=rng,\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    accuracies_sklearn_rf.append(acc)\n",
    "\n",
    "ax0.plot(range(len(accuracies_custom)), accuracies_custom, alpha=0.5)\n",
    "ax0.plot(range(len(accuracies_sklearn)), accuracies_sklearn, alpha=0.5)\n",
    "ax0.plot(range(len(accuracies_sklearn_rf)), accuracies_sklearn_rf, alpha=0.5)\n",
    "plt.legend(['custom', 'sklearn_gbc', 'sklearn_rf'])\n",
    "ax0.set_xlabel('# models used for Boosting ')\n",
    "ax0.set_ylabel('accuracy')\n",
    "print(\n",
    "    f'With a number of {number_of_base_learners} estimators '\n",
    "    f'(boosting stages) we receive an accuracy of {accuracies_custom[-1]*100:.2f}%'\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "ecf786b2d43b092a7580a8865c2697f4846852da4e8971e15cf25fffcac09442"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
